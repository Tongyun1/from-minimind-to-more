为了方便复习，我将其拆分为 **7 大核心板块**，从基础架构到前沿应用（如 RAG、Agent），涵盖了目前面试的高频考点。

考虑到篇幅原因，我就不写回答了。看完我的minimind详解应该能回答一大部分了，然后有不懂了问问AI，面试八股就差不多了。

**大厂面试看中的核心是你对知识的成体系理解，所以不建议死磕八股，而是通过一个项目的实践，建立你对这个领域的系统性认知。**

------

### **第一部分：Transformer 基础架构 (1-20)**

*这部分是基石，主要考察对 Attention 和 Transformer 细节的理解。*

1. 请手写或详细描述 **Self-Attention** 的计算公式及物理含义。
2. 为什么 Transformer 需要 **Multi-Head** Attention？多头有什么作用？
3. Self-Attention 中的 **缩放因子** $\frac{1}{\sqrt{d_k}}$ 也是为了什么？如果不缩放会有什么后果？
4. **Layer Norm** 在 Transformer 中是 Pre-Norm 还是 Post-Norm？在大模型（如 Llama 2/3）中通常用哪种？为什么？
5. Transformer 的 **FFN (Feed Forward Network)** 层的维度通常是如何设计的？引入 SwiGLU 激活函数有什么优势？
6. 解释一下 **Encoder-only** (BERT)、**Decoder-only** (GPT) 和 **Encoder-Decoder** (T5) 架构的区别及适用场景。
7. 为什么现在的 LLM 大多选择 **Decoder-only** 架构？
8. Attention Mask 在 Training 和 Inference 阶段分别是如何起作用的？（Causal Masking）
9. Transformer 的时间复杂度和空间复杂度是多少？瓶颈在哪里？
10. **Bias** 项在现代 LLM（如 Llama）中通常是被保留还是去除？为什么？
11. 什么是 **Cross-Attention**？它主要出现在什么架构中？
12. 解释 **KV Cache** 的原理。为什么它能加速推理？它会带来显存占用的问题吗？
13. 讲一下 **GQA (Grouped Query Attention)** 和 **MQA (Multi-Query Attention)** 的区别及其对性能/显存的影响。
14. Transformer 是否能处理变长序列？Position Embedding 在其中扮演什么角色？
15. 为什么 Attention 矩阵计算出来要做 Softmax？
16. 残差连接（Residual Connection）的作用是什么？如果没有它深层网络会怎样？
17. 解释一下 **DeepSeek** 或 **Mixtral** 使用的 **MoE (Mixture of Experts)** 架构原理。
18. MoE 中的 **负载均衡 (Load Balancing)** 损失函数主要解决什么问题？
19. 什么是 **Sparse Attention**？有哪些常见的变体？
20. Vision Transformer (ViT) 和 NLP 中的 Transformer 有什么本质区别吗？

### **第二部分：位置编码与 Tokenizer (21-30)**

1. 详细解释 **绝对位置编码** (Sinusoidal) 和 **可学习位置编码** 的区别。
2. 什么是 **RoPE (Rotary Positional Embedding)**？它是如何利用复数运算实现相对位置信息的？
3. RoPE 相比于 ALiBi 有什么优缺点？
4. **ALiBi (Attention with Linear Biases)** 是如何实现外推性（Extrapolation）的？
5. 为什么 RoPE 具有长文本外推的能力？
6. 常见的 **Tokenizer** 算法有哪些？（BPE, WordPiece, SentencePiece, Unigram）
7. **BPE (Byte Pair Encoding)** 的训练和推理过程是怎样的？
8. 为什么现在的 LLM（如 Llama, GPT-4）倾向于使用 **Byte-level BPE**？
9. Tokenizer 的词表大小（Vocab Size）对模型性能和训练速度有什么影响？
10. 如何处理 Tokenizer 中的 **OOV (Out of Vocabulary)** 问题？

### **第三部分：预训练 (Pre-training) (31-45)**

*考察对数据处理、Scaling Law 及训练稳定性的理解。*

1. 解释 **Scaling Laws** (Kaplan vs Chinchilla)。在给定算力预算下，如何权衡模型参数量和数据量？
2. 预训练数据的 **去重 (Deduplication)** 有哪些常用算法？（MinHash, SimHash）
3. 为什么数据质量比数据数量更重要？什么是 "Textbooks Are All You Need"？
4. 预训练过程中的 **Loss Spike** (损失尖峰) 通常是由什么引起的？如何缓解？
5. **混合精度训练 (Mixed Precision Training)** 的原理是什么？FP16 和 BF16 有什么区别？为什么大模型训练推荐用 BF16？
6. 解释 **ZeRO (Zero Redundancy Optimizer)** 的三个阶段（ZeRO-1, 2, 3）。
7. 什么是 **Pipeline Parallelism (PP)** 和 **Tensor Parallelism (TP)**？它们分别在什么时候使用？
8. 3D 并行（DP+TP+PP）是如何协同工作的？
9. 预训练任务通常有哪些？（CLM, MLM, PLM, UL2）
10. 什么是 **Curriculum Learning (课程学习)**？在 LLM 预训练中有效吗？
11. 长上下文（Long Context）训练有哪些难点？如何进行 **长文本微调**？
12. **YaRN (Yet another RoPE for Transformers)** 是如何扩展上下文窗口的？
13. 训练大模型时，Batch Size 的大小对收敛速度和最终性能有什么影响？
14. **Gradient Accumulation (梯度累积)** 是用来解决什么问题的？
15. 什么是 **Flash Attention**？简述其通过 IO-Aware 优化加速的原理。

### **第四部分：微调 (SFT & PEFT) (46-60)**

*SFT 是实际工作中接触最多的环节，PEFT 是降本增效的关键。*

1. **SFT (Supervised Fine-Tuning)** 和预训练在数据格式和目标上有什么区别？
2. 详细讲解 **LoRA (Low-Rank Adaptation)** 的数学原理。
3. LoRA 中的 Rank `r` 和 Alpha `\alpha` 参数分别代表什么？如何选择？
4. 为什么 LoRA 只需要训练很小的参数量就能达到不错的效果？
5. **QLoRA** 相比 LoRA 做了哪些改进？（4-bit NormalFloat, Double Quantization, Paged Optimizers）
6. 解释 **Prompt Tuning**, **Prefix Tuning** 和 **P-Tuning** 的区别。
7. **AdaLoRA** 是如何动态分配 Rank 的？
8. 在 SFT 阶段，Instruction (指令) 的多样性和质量哪个更重要？
9. 什么是 **NEFTune**？为什么在 Embedding 上加噪声能提升指令微调效果？
10. 多轮对话数据在训练时通常如何构造？（Loss Masking 怎么做？）
11. **Catastrophic Forgetting (灾难性遗忘)** 是什么？在微调中如何避免？
12. 什么是 **Chat Vector**？
13. 全量微调（Full Fine-tuning）和 LoRA 微调在效果上的主要差距在哪里？
14. 如何在一个模型中通过 Adapter 实现多任务能力？
15. 微调时，Epoch 数通常设为多少？过拟合在 LLM 微调中常见吗？

### **第五部分：对齐 (Alignment: RLHF & DPO) (61-75)**

*让模型“听话”的关键，也是目前算法岗的高阶考点。*

1. 简述 **RLHF (Reinforcement Learning from Human Feedback)** 的三个主要阶段。
2. **Reward Model (奖励模型)** 是如何训练的？Loss Function 是什么？
3. 为什么 RLHF 中需要引入 **KL Divergence (KL 散度)** 惩罚项？
4. 详细解释 **PPO (Proximal Policy Optimization)** 算法在 RLHF 中的作用。
5. 什么是 **DPO (Direct Preference Optimization)**？它相比 PPO 有什么优势？
6. DPO 的 Loss Function 推导逻辑简述（如何消去 Reward Model）。
7. **Rejection Sampling (拒绝采样)** 在对齐中是如何使用的？与 PPO 相比效果如何？
8. **IPO (Identity-PO)** 和 **KTO (Kahneman-Tversky Optimization)** 是什么？
9. RLHF 会导致 **Alignment Tax (对齐税)** 吗？即模型能力下降。
10. 数据标注中，偏好数据（Preference Data）通常是 A vs B 还是打分制？为什么？
11. 什么是 **Constitutional AI (宪法 AI)**？
12. 如何解决 Reward Hacking (奖励模型欺骗) 问题？
13. 为什么说 RLHF 的本质可能只是调整了输出的概率分布，而没有注入新知识？
14. 在 DPO 训练中，Reference Model 的作用是什么？
15. 现在的趋势是 RLHF 还是 RLAIF (AI Feedback)？

### **第六部分：推理优化与量化 (76-88)**

*工程落地必问，考察如何把模型跑得快、省显存。*

1. 常见的 **解码策略 (Decoding Strategy)** 有哪些？（Greedy, Beam Search, Top-k, Top-p）
2. 解释 **Temperature** 和 **Top-p (Nucleus Sampling)** 对生成结果多样性的影响。
3. 什么是 **Speculative Decoding (投机采样/推测解码)**？原理是什么？
4. **vLLM** 框架的核心技术 **PagedAttention** 是解决了什么问题？
5. 常见的量化方法有哪些？（PTQ vs QAT）
6. 解释 **GPTQ** 和 **AWQ (Activation-aware Weight Quantization)** 的区别。
7. 为什么大模型的权重可以被量化到 4-bit 甚至更低而不严重掉点？
8. 什么是 **KV Cache Quantization**？会有精度损失吗？
9. **Continuous Batching** 是如何提升推理吞吐量的？
10. 推理时的 **TTFT (Time To First Token)** 和 **TPOT (Time Per Output Token)** 指标分别代表什么？
11. 如何估算一个 7B 模型在 FP16 下推理所需的显存大小？
12. **TensorRT-LLM** 做了哪些优化？
13. 什么是 **Static KV Cache** 和 **StreamingLLM**（针对无限长对话）？

### **第七部分：RAG、Agent 与 评测 (89-100)**

*应用层问题，考察解决幻觉和复杂任务的能力。*

1. 简述 **RAG (Retrieval-Augmented Generation)** 的标准流程。
2. RAG 中常见的 **Chunking (切分)** 策略有哪些？
3. 如何解决 RAG 中的 **"Lost in the Middle"** 现象？
4. 什么是 **GraphRAG**？知识图谱如何辅助 RAG？
5. 什么是 **Vector Database**？HNSW 索引原理是什么？
6. 解释 **CoT (Chain of Thought)** 的原理。为什么它能提升推理能力？
7. **ReAct (Reasoning + Acting)** 框架是如何工作的？
8. 常见的 LLM 评测榜单（MMLU, GSM8K, HumanEval, C-Eval）分别侧重考察什么能力？
9. 什么是 **Needle In A Haystack (大海捞针)** 测试？
10. 幻觉 (Hallucination) 产生的根本原因可能有哪些？
11. **Function Calling (工具调用)** 是如何训练的？
12. 谈谈你对未来 **LLM -> System 2 (慢思考)** 发展的看法。

------

### **建议复习策略**

1. **从原理出发**：对于 1-20 题（Transformer）和 46-50 题（LoRA），不要死记硬背，要能手推公式或画出架构图。
2. **关注前沿**：DPO、MoE、Flash Attention v2、vLLM 是现在面试非常喜欢问的“加分项”。