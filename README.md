<img width="1314" height="432" alt="Image" src="https://github.com/user-attachments/assets/c2b6252a-a09d-4533-933a-a6ff4d420dc9" />

# From Minimind to More 🚀

> 感谢Minimind原作者的无私开源！
>
> 深入探索大语言模型：从底层基石到高层架构，从理论原理到工程实践。

## 📖 项目简介 | Introduction

本项目是我个人基于https://github.com/jingyaogong/minimind 的学习笔记与思考。我从Minimind出发，系统性梳理了其中涉及到的知识点，并附带了相关的其他要点。**我希望本项目能够不仅让读者看懂Minimind，更能对大模型的技术体系建立一个全面的insight**。

这里不仅包含了我对Minimind用到的**技术的详细解析**，**源码的超详细注释**，也整理了**面向求职的面试题库**。无论你是想深入了解 Minimind 架构与训练的细节，还是准备相关领域的面试，希望这里的内容能**最大化减少你到处找资料的次数**，并给你带来启发。

🚧 **当前状态**：项目持续更新中，目前主要覆盖架构与基础部分，[算法篇]正在筹备中...

网页对md的解析可能有错误，如遇公式或者图片的问题请下载到本地查看。

## 📚 内容导航 | Table of Contents

### 🏗️ 第一部分：基石与原理 (Foundations)
万丈高楼平地起，这里是理解 LLM 的起点。
- [x] **Tokenization**：[基石：关于 Tokenizer 你所需要知道的一切](./基石：关于Tokenizer你所需要知道的一切.md)
- [x] **整体设计**：[基石：Minimind 的设计目录](./架构篇：Minimind的设计目录.md)
- [x] **Embeddings**：[基石：语义的几何与时空的折叠：Embedding与位置编码](./基石：语义的几何与时空的折叠：Embedding与位置编码.md)

### 🏛️ 第二部分：核心架构 (Architecture)
深入 Transformer 及其变体的内部构造，解析最前沿的模型设计。
- [x] **归一化技术**：[架构篇：大语言模型归一化技术：原理、演进与前沿架构](./架构篇：大语言模型归一化技术：原理、演进与前沿架构.md)
- [x] **性能优化**：[架构篇：最常见的大模型优化方法：从KV Cache到Flash Attention](./架构篇：最常见的大模型优化方法：从KV%20Cache到Flash%20Attention.md)
- [x] **混合专家模型**：[架构篇：混合专家模型（MoE）：架构演进、核心算法与工程实践](./架构篇：混合专家模型（MoE）：架构演进、核心算法与工程实践.md)
- [ ] **搭建我们自己的大模型**：[架构篇：超级拼装](./架构篇：超级拼装.md)
- [x] **(可选阅读)**：[大规模语言模型推理与训练优化机制](./可选：大规模语言模型推理与训练优化机制.md)

### 🧠 第三部分：算法与演进 (Algorithms) - *Coming Soon*
*本章节正在撰写中，将涵盖预训练算法、微调策略（SFT/RLHF）等核心算法细节。*
- [ ] 预训练算法
- [ ] SFT算法
- [ ] RL算法

### 🎓 第四部分：求职与实战 (Career & Practice)

*本章节正在撰写中，讲涵盖我个人对大模型求职的笔记与经验。*

- [x] **面试八股**：[大模型八股 100 问](./大模型八股100问.md)
- [ ] **面试题库**：正在收集

---

## 📅 更新计划 | Roadmap

- **Phase 1 (Completed)**: 完成基础组件（Tokenizer, Embeddings）与核心架构（MoE, Normalization, KV Cache）的解析。
- **Phase 2 (In Progress)**: 完善 [算法篇]，深入探讨训练机制等算法细节。
- **Phase 3 (Planned)**: 补充更多我个人的实战案例。

## 🤝 交流与致谢
如果你发现文章中有任何错误，或者有更好的见解，欢迎提交 Issue 或 PR。

再次感谢Minimind原作者的无私开源。同时，感谢https://github.com/hans0809/MiniMind-in-Depth ，我从该项目中学到了很多。
