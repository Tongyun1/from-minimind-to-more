# From Minimind to More 🚀

> 感谢Minimind原作者的无私开源！
>
> 深入探索大语言模型：从底层基石到高层架构，从理论原理到工程实践。

## 📖 项目简介 | Introduction

本项目是我个人基于https://github.com/jingyaogong/minimind的学习笔记与思考。我从Minimind出发，系统性梳理了其中涉及到的知识点，并附带了相关的其他要点。希望本项目不仅可以帮助你深度理解Minimind，更能对大语言模型知识有更多的了解！

这里不仅包含了我对 LLM 关键技术的深度思考与原理解析，也整理了面向求职的面试题库。无论你是想深入了解 Transformer 架构的细节，还是准备相关领域的面试，希望这里的内容能给你带来启发。

🚧 **当前状态**：项目持续更新中，目前主要覆盖架构与基础部分，[算法篇]正在筹备中...

## 📚 内容导航 | Table of Contents

### 🏗️ 第一部分：基石与原理 (Foundations)
万丈高楼平地起，这里是理解 LLM 的起点。
- [x] **Tokenization**：[基石：关于 Tokenizer 你所需要知道的一切](./基石：关于Tokenizer你所需要知道的一切.md)
- [x] **Embeddings**：[基石：语义的几何与时空的折叠：Embeddings](./基石：语义的几何与时空的折叠：Embeddi....md)

### 🏛️ 第二部分：核心架构 (Architecture)
深入 Transformer 及其变体的内部构造，解析最前沿的模型设计。
- [x] **整体设计**：[架构篇：Minimind 的设计架构目录](./架构篇：Minimind的设计架构目录.md)
- [x] **混合专家模型**：[架构篇：混合专家模型（MoE）：架构演进、核心原理](./架构篇：混合专家模型（MoE）：架构演进、核....md)
- [x] **归一化技术**：[架构篇：大语言模型归一化技术：原理、演进](./架构篇：大语言模型归一化技术：原理、演进....md)
- [x] **性能优化**：[架构篇：最常见的大模型优化方法：从 KV Cache 到其它](./架构篇：最常见的大模型优化方法：从KV%20Cac....md)
- [x] **(可选阅读)**：[大规模语言模型推理与训练优化机制](./可选：大规模语言模型推理与训练优化机制.md)

### 🧠 第三部分：算法与演进 (Algorithms) - *Coming Soon*
*本章节正在撰写中，将涵盖预训练算法、微调策略（SFT/RLHF）等核心算法细节。*
- [ ] 预训练目标与损失函数
- [ ] 位置编码的演进（RoPE, ALiBi 等）
- [ ] ...

### 🎓 第四部分：求职与实战 (Career & Practice)
- [x] **面试宝典**：[大模型八股 100 问](./大模型八股100问.md)

---

## 📅 更新计划 | Roadmap

- **Phase 1 (Completed)**: 完成基础组件（Tokenizer, Embeddings）与核心架构（MoE, Normalization, KV Cache）的解析。
- **Phase 2 (In Progress)**: 完善 [算法篇]，深入探讨训练机制与位置编码等算法细节。
- **Phase 3 (Planned)**: 补充更多实战代码与工程化落地的案例。

## 🤝 交流与致谢
如果你发现文章中有任何错误，或者有更好的见解，欢迎提交 Issue 或 PR。

---
*Created by ZhangMingming*
