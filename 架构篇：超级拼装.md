# 架构篇：超级拼装

**这一章并不仅仅是把前文的几个模块给拼起来，还包含了许多细节。**

## 1. 核心模块详解

MiniMind 由多个高度模块化的组件构成，以下再次总结各核心类的功能与设计原理。如果有不懂的地方请回过头到前面的几章去找对应内容。

### 1.1 基础组件

- **MiniMindConfig (`PretrainedConfig`)**
  - **作用**：模型的“控制台”。定义了维度（hidden_size）、层数、头数等基础参数。
  - **特色**：集成了 **YaRN**（位置编码外推）和 **MoE**（混合专家）的开关与参数配置，支持从 Dense 模式无缝切换到 Sparse 模式。
- **RMSNorm (`Root Mean Square Layer Normalization`)**
  - **作用**：层归一化。
  - **原理**：相比标准的 LayerNorm，RMSNorm 移除了中心化操作（不减均值），仅保留缩放。
  - **优势**：计算量更小，数值稳定性更好，是目前主流大模型（如 LLaMA, Gemma）的标准配置。

### 1.2 注意力机制 (Attention)

- **Attention 类**
  - **作用**：实现模型对上下文的理解。
  - **核心技术**：
    - **GQA (Grouped Query Attention)**：Query 头数多于 Key/Value 头数（`num_attention_heads` vs `num_key_value_heads`）。通过 `repeat_kv` 函数复制 KV，显著降低了推理时的显存占用和 KV Cache 大小。
    - **RoPE + YaRN**：集成了旋转位置编码。特别引入了 **YaRN** 算法，通过动态调整频率（`beta_fast`, `beta_slow`），让模型在未重新训练的情况下也能处理比训练长度更长的序列（最长支持 32k）。
    - **Flash Attention**：自动检测环境，若 PyTorch 版本支持，直接调用底层优化的 `F.scaled_dot_product_attention`，大幅提升训练和推理速度。

### 1.3 前馈网络 (FeedForward & MoE)

这是 MiniMind 区别于普通 Transformer 的核心区域，支持两种模式：

- **FeedForward (标准 Dense 模式)**
  - **架构**：采用 **SwiGLU** 激活函数。
  - **流程**：输入经过门控投影（Gate）和上投影（Up），相乘后过 Swish 激活，最后经下投影（Down）输出。
  - **优势**：相比传统的 ReLU+FFN，SwiGLU 具有更好的收敛性和性能。
- **MOEFeedForward (混合专家模式)**
  - **架构**：DeepSeek-V2/V3 风格的 MoE 架构。
  - **组成**：
    1. **Shared Experts (共享专家)**：`n_shared_experts` 个，处理**所有** Token，旨在捕获通用知识。
    2. **Routed Experts (路由专家)**：`n_routed_experts` 个，由门控网络动态选择。
    3. **MoEGate (门控)**：计算每个 Token 对各个专家的评分，选出 Top-K 个专家，并计算**辅助损失 (Aux Loss)** 以防止负载不均。
  - **流程**：`Output = 共享专家输出 + Σ(路由专家输出 * 门控权重)`。

### 1.4 模型骨架

- **MiniMindBlock**
  - **作用**：Transformer 的标准层。
  - **结构**：采用 **Pre-Norm** 设计。
    - `Input -> Norm -> Attention -> Residual Add`
    - `Input -> Norm -> FFN/MoE -> Residual Add`
- **MiniMindForCausalLM**
  - **作用**：完整的语言模型封装。
  - **权重共享 (Weight Tying)**：`embed_tokens`（输入层）与 `lm_head`（输出层）共享同一个权重矩阵。这是一个显著的压缩策略，能大幅减少参数量（尤其是在词表很大时）。

## 2. 整体架构组装原理

![structure](https://github.com/jingyaogong/minimind/raw/master/images/LLM-structure.png)

![structure-moe](https://github.com/jingyaogong/minimind/raw/master/images/LLM-structure-moe.png)

MiniMind 的数据流转遵循标准的自回归（Auto-Regressive）生成范式，其组装逻辑如下：

1. **输入处理**：
   - 输入 Token IDs (`input_ids`) 进入 `MiniMindModel`。
   - 通过 `embed_tokens` 转换为稠密向量（Hidden States）。
2. **位置编码预备**：
   - 根据配置（RoPE 或 YaRN），预先计算好正弦和余弦频率矩阵 (`freqs_cos`, `freqs_sin`)。
3. **深层网络堆叠 (The Loop)**：
   - Hidden States 依次穿过 `num_hidden_layers` 个 `MiniMindBlock`。
   - 在每一层中：
     - 数据先被 `RMSNorm` 归一化。
     - 进入 `Attention`，结合 RoPE 进行上下文聚合，利用 KV Cache 加速。
     - 残差连接后，再次 `RMSNorm`。
     - 进入 `FeedForward` 或 `MOEFeedForward` 进行特征变换（如果是 MoE，此时会计算路由和辅助损失）。
     - 再次残差连接。
4. **输出生成**：
   - 经过所有层后，进行最后的 `RMSNorm`。
   - 在 `MiniMindForCausalLM` 中，通过 `lm_head` 将向量映射回词表维度 (`vocab_size`)。
   - 计算 CrossEntropy Loss（训练时）或输出 Logits（推理时）。

## 3. Minimind的代码实现详细解析

这是Minimind的模型组装的代码实现，其中有详细注释。请把注释全部看懂，里面包含了很多知识点

```python
class MiniMindModel(nn.Module):
    """
    MiniMind 主模型核心类

    这是 Transformer 的 Decoder-Only 架构实现（类似 LLaMA 结构）。
    它负责将输入的 Token IDs 转换为深层的语义特征表示（Hidden States）。

    维度符号约定：
        B (Batch Size): 批次大小 (推理时通常为 1)
        S (Seq Length): 当前输入的序列长度 (训练时为全长，推理Decoding阶段为 1)
        H (Hidden Size): 隐藏层维度 (如 512)
        V (Vocab Size): 词表大小 (如 6400)
        L (Layers): 层数 (如 8)
        HD (Head Dim): 单个注意力头的维度 (H // num_heads)
        MaxPos: 最大支持序列长度

    主要流程：
    Input IDs -> Embeddings -> [Transformer Blocks x L] -> RMSNorm -> Output Hidden States
    """
    def __init__(self, config: MiniMindConfig):
        """
        初始化模型结构

        Args:
            config: 包含所有超参数的配置对象
        """
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers

        # ========== 1. 词嵌入层 (Embedding) ==========
        # 将离散的 Token ID 映射为稠密向量
        # 形状: [V, H]
        # 注意：在 MiniMindForCausalLM 中，这个权重通常与输出层的 lm_head 共享 (Weight Tying)
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # Dropout 层，用于防止过拟合
        self.dropout = nn.Dropout(config.dropout)

        # ========== 2. 堆叠 Transformer 层 (Layers) ==========
        # 使用 ModuleList 存储 L 个 MiniMindBlock
        # 每个 Block 包含 Attention 和 FFN/MoE
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])

        # ========== 3. 最终归一化层 (Final Norm) ==========
        # 在输出之前进行最后一次 RMSNorm，这是 LLaMA 架构的标准做法
        # 形状: [H]
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # ========== 4. 预计算 RoPE 位置编码 (Precompute RoPE) ==========
        # 预先计算所有可能位置的 Cos 和 Sin 值，避免前向传播时重复计算
        # freqs_cos/sin 形状: [MaxPos, HD]
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads,  # Head Dim
            end=config.max_position_embeddings,                    # 最大位置索引 (如 32768)
            rope_base=config.rope_theta,                           # RoPE 基频
            rope_scaling=config.rope_scaling                       # YaRN 外推配置
        )
        
        # 将频率表注册为 buffer
        # buffer 不会被视为模型参数 (parameter)，不参与梯度更新，但会随模型权重文件保存
        # persistent=False 表示这些值可以根据 config 动态重新计算，不强制依赖权重文件
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        """
        前向传播逻辑

        Args:
            input_ids: 输入序列 [B, S]。
                       训练时 S 是整个句子长度；
                       推理 Decoding 阶段 S 通常为 1。
            attention_mask: 掩码 [B, S]。
            past_key_values: 历史 KV 缓存列表。
                             List 长度为 L，每个元素是 (K, V) 元组。
                             K/V 形状: [B, Past_Len, Num_KV_Heads, HD]。
            use_cache: 是否开启 KV Cache 加速 (推理时为 True)。

        Returns:
            hidden_states: [B, S, H] 模型输出特征
            presents: 新的 KV Cache 列表
            aux_loss: MoE 负载均衡辅助损失
        """
        # 获取输入的 Batch Size 和 Sequence Length
        # 注意：推理 Decoding 阶段，seq_length 始终为 1
        batch_size, seq_length = input_ids.shape

        # ========== KV Cache 兼容性处理 ==========
        # 如果传入的是 Hugging Face 新版的高级 Cache 对象 (含有 .layers 属性)
        # MiniMind 暂时不支持，为了防止报错，强制清空缓存 (安全降级)
        if hasattr(past_key_values, 'layers'):
            past_key_values = None
        
        # 初始化 past_key_values
        # 如果没有缓存 (Prefill 阶段或训练阶段)，初始化为全 None 的列表
        past_key_values = past_key_values or [None] * len(self.layers)

        # ========== 计算起始位置 (start_pos) ==========
        # 这里的逻辑是确定当前输入的 Token 在整篇文章中的绝对位置索引
        # 1. 如果有缓存 (past_key_values[0] 不为 None):
        #    说明是推理的 Decoding 阶段。
        #    past_key_values[0][0] 是第 0 层的 Key Tensor，形状 [B, Past_Len, H_kv, HD]
        #    .shape[1] 就是 Past_Len (历史已经处理过的 Token 数量)
        #    这也是当前新 Token 的起始索引。
        # 2. 如果没有缓存:
        #    说明是 Prefill 阶段或训练阶段，从第 0 个位置开始。
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # ========== Token Embedding ==========
        # 将 ID 转换为向量: [B, S] -> [B, S, H]
        # 此时 hidden_states 包含了语义信息，但还没有位置信息
        hidden_states = self.dropout(self.embed_tokens(input_ids))

        # ========== 提取位置编码 (RoPE Slicing) ==========
        # 根据绝对位置 start_pos 和当前长度 seq_length，从预计算的表中切片
        # 切片范围: [start_pos : start_pos + seq_length]
        # 
        # 场景 A (训练/Prefill): start_pos=0, seq_len=N -> 取出前 N 个位置编码
        # 场景 B (推理 Decoding): start_pos=N, seq_len=1 -> 仅取出第 N 个位置的编码 (长度为 1)
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # ========== 逐层前向传播 ==========
        presents = [] # 用于收集每一层新的 KV Cache
        
        # zip 组合：将 模型层对象 与 该层对应的历史缓存 配对
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            # 进入 Transformer Block
            # 输入: hidden_states [B, S, H]
            # 输出: 
            #   hidden_states: 更新后的特征 [B, S, H]
            #   present: 当前层更新后的 KV Cache (包含历史+当前), 形状 [B, Past_Len+S, H_kv, HD]
            hidden_states, present = layer(
                hidden_states,
                position_embeddings, # 传入切片好的位置编码
                past_key_value=past_key_value, # 传入该层的历史缓存
                use_cache=use_cache, # 指示 Block 是否需要返回缓存
                attention_mask=attention_mask
            )
            presents.append(present)

        # ========== 最终归一化 ==========
        # 经过所有层后，进行最后一次 RMSNorm
        # [B, S, H] -> [B, S, H]
        hidden_states = self.norm(hidden_states)

        # ========== 汇总 MoE 辅助损失 ==========
        # 检查每一层，如果是 MoE 层 (MOEFeedForward)，提取其 aux_loss
        # 将所有层的 aux_loss 相加，用于训练时的反向传播
        # 如果没有使用 MoE，总 aux_loss 为 0
        aux_loss = sum(
            [l.mlp.aux_loss for l in self.layers if isinstance(l.mlp, MOEFeedForward)],
            hidden_states.new_zeros(1).squeeze()
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    """
    MiniMind 因果语言模型 (Causal Language Model)
    
    这是面向最终任务（文本生成）的顶层封装类。
    
    架构组成：
        Input IDs -> [MiniMindModel] -> Hidden States -> [LM Head] -> Logits
    
    关键特性：
        1. 权重共享 (Weight Tying): 输入 Embedding 和输出 LM Head 共享同一份参数，显著减少显存。
        2. 推理优化 (Logits Slicing): 支持只计算最后一个 Token 的 Logits，避免全量计算。
        3. 训练并行 (Parallel Training): 利用 Mask 实现一次性计算所有 Token 的 Loss。
    """
    config_class = MiniMindConfig  # 指定配置类，Hugging Face 框架自动加载机制需要

    def __init__(self, config: MiniMindConfig = None):
        """
        初始化模型结构
        """
        # 如果没有传入 config，则实例化一个默认配置
        self.config = config or MiniMindConfig()
        
        # 初始化父类 PreTrainedModel (负责权重加载、保存、下载等)
        super().__init__(self.config)
        
        # ========== 1. 骨干网络 (Backbone) ==========
        # 实例化纯 Transformer Decoder，负责提取深层语义特征
        # 输入: [Batch, Seq_Len] -> 输出: [Batch, Seq_Len, Hidden_Size]
        self.model = MiniMindModel(self.config)
        
        # ========== 2. 语言模型头 (LM Head) ==========
        # 这是一个线性投影层 (Linear Layer)
        # 作用: 将高维特征向量 (Hidden State) 映射回词表空间 (Vocab Space)
        # 形状: [Hidden_Size] -> [Vocab_Size]
        # bias=False: 现代大模型 (LLaMA等) 通常不使用偏置项，以提升数值稳定性
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        
        # ========== 3. 权重共享 (Weight Tying) ==========
        # [重要优化] 将 Input Embedding 的权重指针指向 LM Head 的权重
        # 物理意义: 语义上，“输入一个词”和“预测一个词”使用的是同一个语义空间。
        # 显存优势: 词表通常很大 (如 64k)，权重共享能节省大量参数 (Hidden * Vocab)。
        self.model.embed_tokens.weight = self.lm_head.weight

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        """
        前向传播 (支持 训练 和 推理 两种模式)
        
        Args:
            input_ids: 输入序列 [Batch, Seq_Len]。
                       - 训练时: 是一整句话 (Seq_Len = N)。
                       - 推理时(Decoding): 通常只是最新生成的那个词 (Seq_Len = 1)。
            attention_mask: 掩码 [Batch, Seq_Len] (1=有效, 0=padding)。
            labels: 标签序列 [Batch, Seq_Len]。
                    - 如果提供此参数，模型会计算 Loss (训练模式)。
                    - 如果为 None，只返回 Logits (推理模式)。
            past_key_values: KV Cache 列表。
                    - 用于存储历史 Token 的 Key/Value，避免重复计算。
            use_cache: 是否返回更新后的 KV Cache (推理时开启)。
            logits_to_keep: 【性能优化参数】
                    - 0 (默认): 计算所有 Token 的 Logits (训练时必须选这个)。
                    - 1 (常用): 只计算最后一个 Token 的 Logits (推理生成时用)。
                    原理: 避免在 lm_head 上进行无用的矩阵乘法计算。
        
        Returns:
            CausalLMOutputWithPast: 包含 loss, logits, hidden_states, past_key_values, aux_loss
        """
        
        # ========== 步骤 1: 骨干网络特征提取 ==========
        # 数据流经 Transformer 的所有层
        # hidden_states: [Batch, Seq_Len, Hidden_Size]
        # past_key_values: 包含了当前步新生成的 KV Cache
        # aux_loss: 如果使用了 MoE，这里会返回负载均衡损失；否则为 0
        hidden_states, past_key_values, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        
        # ========== 步骤 2: Logits 计算范围优化 (Logits Slicing) ==========
        # lm_head 的计算量是 O(Seq_Len * Hidden * Vocab)，非常巨大。
        # 在推理时，我们只需要最后一个词的预测结果，不需要前文的预测。
        
        if isinstance(logits_to_keep, int):
            # 整数模式
            # logits_to_keep = 1 -> slice(-1, None) -> 取最后 1 个
            # logits_to_keep = 0 -> slice(None)     -> 取全部 (训练时)
            slice_indices = slice(-logits_to_keep, None) if logits_to_keep > 0 else slice(None)
        else:
            # 张量模式 (高级用法，指定特定位置)
            slice_indices = logits_to_keep
        
        # 对 Hidden States 进行切片，只保留需要计算的部分
        # 推理时: [Batch, 100, Hidden] -> [Batch, 1, Hidden]
        # 训练时: [Batch, 100, Hidden] -> [Batch, 100, Hidden]
        sliced_hidden_states = hidden_states[:, slice_indices, :]
        
        # ========== 步骤 3: 映射到词表 (Projection) ==========
        # 执行矩阵乘法: X @ W.T
        # logits 形状: [Batch, Sliced_Len, Vocab_Size]
        # 这里的 logits 是未归一化的概率 (Log-odds)
        logits = self.lm_head(sliced_hidden_states)

        # ========== 步骤 4: 计算损失 (仅训练模式) ==========
        loss = None
        if labels is not None:
            # 因果语言模型的核心逻辑: "Shift Prediction" (位移预测)
            # 目标: 第 t 个时间步的 Logit，应该预测第 t+1 个时间步的 Label。
            
            # [Input]:  A  B  C  D
            # [Target]: B  C  D  E
            
            # shift_logits: 去掉最后一个 Logit (因为它预测的是 E，但 Input 只有到 D)
            # 形状: [Batch, Seq_Len-1, Vocab]
            shift_logits = logits[..., :-1, :].contiguous()
            
            # shift_labels: 去掉第一个 Label (因为 A 之前没有 Logit 预测它)
            # 形状: [Batch, Seq_Len-1]
            shift_labels = labels[..., 1:].contiguous()
            
            # 计算交叉熵损失 (Cross Entropy)
            # view(-1): 将 Batch 和 Seq 维度展平，变成 [Total_Tokens, Vocab] 以适配 Loss 函数
            # ignore_index=-100: 忽略标签为 -100 (Padding) 的位置，不计算梯度
            loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
                ignore_index=-100
            )

        # ========== 步骤 5: 封装输出 ==========
        # 使用 Hugging Face 标准格式返回，确保兼容性
        output = CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=past_key_values,
            hidden_states=hidden_states
        )
        
        # [MoE 特有] 挂载辅助损失
        # 训练循环中通常写法: total_loss = output.loss + alpha * output.aux_loss
        output.aux_loss = aux_loss
        
        return output
```

## 4. 与主流大模型架构的异同

MiniMind 在设计上明显对标了 **LLaMA** 系列，同时吸纳了 **DeepSeek** 和 **Gemma** 的部分特性。

### 3.1 相同点 (主流标配)

- **基础骨架**：都是基于 Transformer Decoder Only 架构。
- **归一化**：均使用 **RMSNorm** 且采用 **Pre-Norm** 方式（LLaMA 标准）。
- **激活函数**：均采用 **SwiGLU**（GLU 变体），这是目前公认最高效的大模型激活函数。
- **位置编码**：基础版均使用 **RoPE**（旋转位置编码）。

### 3.2 不同点与特色 (MiniMind 的改进)

| **特性**          | **LLaMA 2/3 (原生)**                     | **MiniMind 实现**                     | **评价 / 优势**                                              |
| ----------------- | ---------------------------------------- | ------------------------------------- | ------------------------------------------------------------ |
| **长文本处理**    | 标准 RoPE (线性插值需微调)               | **YaRN (无需训练的外推)**             | MiniMind 通过 YaRN 算法，可以在不进行长文本微调的情况下，直接增强模型处理长序列的能力（Extrapolation）。 |
| **MoE 架构**      | 无 (LLaMA 是 Dense 模型)                 | **DeepSeek 式 MoE (Shared + Routed)** | MiniMind 实现了类似 DeepSeek-V2 的架构，引入了“共享专家”概念，比传统的 Mixtral MoE (仅有路由专家) 训练更稳定，知识表达更强。 |
| **权重共享**      | 不共享 (Untied Embeddings)               | **共享 (Tied Embeddings)**            | 类似 **Gemma** 或 **GPT-2** 的设计。输入 Embedding 和输出 Head 共享权重，显著减少显存占用，适合中小参数规模的模型。 |
| **KV Cache 优化** | GQA (仅在 LLaMA-2-70B 和 LLaMA-3 中使用) | **全系支持 GQA**                      | 无论模型大小，MiniMind 均支持分组查询注意力 (GQA)，大幅降低推理时的 KV Cache 显存压力，提高吞吐量。 |

### 总结

MiniMind 是一个**“麻雀虽小，五脏俱全”**的现代 LLM 实现。它不是简单的 LLaMA 复刻，而是结合了：

1. **DeepSeek** 的高效 MoE 路由机制（Shared + Routed Experts）。
2. **YaRN** 的先进长窗口技术。
3. **Gemma** 的权重共享轻量化设计。
4. **LLaMA** 的稳健基础骨架 (RMSNorm, SwiGLU)。

**至此，Minimind的模型架构已经全部完成。希望你对大模型的基础架构已经有了比较好了insight。接下来，我们会进入算法篇，去探究Minimind的预训练，SFT，RL算法。**